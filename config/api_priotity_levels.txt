    #LLMs available through our server (smaller LLMs do not have a context window large enough to reason)
         



        #api > gemini2.5-flash:5b



1 = highest , 5 = lowest

priority 1:
api > openai-gpt5
api > anseble api
# gpt-oss:120b          f7f8e2f8f4e0    65 GB     2 weeks ago     context length      131,072 #best local option
# deepseek-r1:70b       d37b54d01a76    42 GB     2 weeks ago     context length      131,072  # should run to compare deepseeks capabilties, not good for this task



priority: 2

api > gemini2.5-flash:5b 
# gpt-oss:20b           aa4295ac10c3    13 GB     3 weeks ago     context length      131,072 #worthy of testing
# llama3-gradient:70b   b5d6e9d0ae61    39 GB     36 seconds ago  context length      1,048,576   #run but run last, with same or a bit more of a window than openai




priority: 3
# api > llama3.3:70b          a6eb4748fd29    42 GB     46 hours ago    context length      131,072 #long time outs needed to test , still need to see format





priority 4:


api > phi4-reasoning:14b gets responses and does a below avg job, can hit time outs

# qwen3:32b             030ee887880f    20 GB     3 weeks ago     context length      40,960  #low priority decent responses, slow run time

# gemma3:27b            a418f5838eaf    17 GB     3 days ago      context length      131,072 # solid, shows the importance of a context window

api > reflection:70b    #does not provide much value running it


do not run:

api > qwen3:0.6b    too small, no benefit if ran

# qwen:72b              e1c64582de5c    41 GB     2 days ago      context length      32,768  # do not use, responds with blanks or gibberish.
    if ran save till the end, shows a lot of the context winodw, its parameters 


# gpt-oss:latest        aa4295ac10c3    13 GB     2 weeks ago     context length      131,072  #redundant

# llama3.1:70b          711a9e8463af    42 GB     3 weeks ago     context length      131,072 #use llama 3.3, redundant

#magistral too long of TAT for no high end reults or comparison 